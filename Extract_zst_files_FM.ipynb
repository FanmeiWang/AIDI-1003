{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9cbce3-3017-455a-9698-835bd82b731e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created by Fanmei Wang, March 30, 2025\n",
    "\n",
    "import os\n",
    "import json\n",
    "import zstandard as zstd\n",
    "\n",
    "# ========== Configuration ==========\n",
    "\n",
    "# Input file\n",
    "input_file = r\"path\\RS_2024-02.zst\"\n",
    "\n",
    "# Output file (only save posts from target subreddits)\n",
    "output_file = \"path/extracted_RS_2024-02_filtered.jsonl\"\n",
    "\n",
    "# âœ… Target subreddits to extract (lowercase)\n",
    "target_subreddits = {\"immigrationcanada\", \"canadaimmigrant\"}\n",
    "\n",
    "# ========== Script starts ==========\n",
    "\n",
    "file_size = os.path.getsize(input_file)\n",
    "\n",
    "print(f\"Starting extraction: {input_file}\")\n",
    "print(f\"Compressed file size: {file_size / (1024*1024):.2f} MB\")\n",
    "print(f\"Filtering for subreddits: {target_subreddits}\")\n",
    "\n",
    "lines_read = 0        # Number of lines read and parsed\n",
    "lines_written = 0     # Number of matching posts written\n",
    "bytes_processed = 0   # Number of compressed bytes processed\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(input_file, \"rb\") as compressed_file:\n",
    "        dctx = zstd.ZstdDecompressor(max_window_size=2_147_483_648)  # 2 GiB\n",
    "        with dctx.stream_reader(compressed_file) as reader:\n",
    "            buffer = b\"\"\n",
    "            chunk_size = 2**20  # Read 1MB at a time\n",
    "\n",
    "            while True:\n",
    "                chunk = reader.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "\n",
    "                bytes_processed += len(chunk)\n",
    "                buffer += chunk\n",
    "\n",
    "                lines = buffer.split(b\"\\n\")\n",
    "                buffer = lines.pop()\n",
    "\n",
    "                for line in lines:\n",
    "                    lines_read += 1\n",
    "\n",
    "                    if lines_read % 100000 == 0:\n",
    "                        percent = (bytes_processed / file_size) * 100\n",
    "                        print(f\"Processed {lines_read:,} lines, {percent:.2f}% of compressed file, matched: {lines_written:,}\")\n",
    "\n",
    "                    try:\n",
    "                        post = json.loads(line)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "\n",
    "                    # âœ… Filter by target subreddit (case-insensitive)\n",
    "                    sr = post.get(\"subreddit\", \"\").lower()\n",
    "                    if sr in target_subreddits:\n",
    "                        out.write(json.dumps(post) + \"\\n\")\n",
    "                        lines_written += 1\n",
    "\n",
    "print(\"\\nâœ… Extraction completed!\")\n",
    "print(f\"ðŸ”¢ Total lines read    : {lines_read:,}\")\n",
    "print(f\"âœ… Total posts matched : {lines_written:,}\")\n",
    "print(f\"ðŸ“„ Output file         : {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
